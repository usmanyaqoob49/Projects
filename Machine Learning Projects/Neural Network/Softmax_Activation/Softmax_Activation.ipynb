{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #we will do exponential of every element of z\n",
    "    ez = np.exp(z)\n",
    "    a = ez / np.sum(ez)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Loss is for one example while Cost covers all examples.\n",
    "\n",
    "\n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "centers = [[-5,2],\n",
    "           [-2,2],\n",
    "           [1,2],\n",
    "           [5,-2]]\n",
    "x_train,y_train = make_blobs(n_samples = 2000, centers = centers,cluster_std = 1.0, random_state = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the compile directive.\n",
    "\n",
    "The loss function is SparseCategoricalCrossentropy. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Prefered Softmax Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbeac68bc20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbeac68bc20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.1976\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7460\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5164\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3753\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3055\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2687\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2461\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2303\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2203\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbeac5e6210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(25,activation = 'relu'),\n",
    "    Dense(15,activation = 'relu'),\n",
    "    Dense(4,activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001),)\n",
    "\n",
    "model.fit(x_train,y_train,epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the softmax is integrated into the output layer, the output is a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3200734e-04 1.5484955e-02 9.5769781e-01 2.6685284e-02]\n",
      " [9.5720732e-01 4.2127814e-02 4.9191201e-04 1.7294526e-04]]\n",
      "largest value  0.9999993\n",
      "smallest value  1.5161074e-12\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(x_train)\n",
    "print(p_nonpreferred [:2])\n",
    "print('largest value ', np.max(p_nonpreferred))\n",
    "print('smallest value ',np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefered Softmax Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as *logits*. The loss function has an additional argument: `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbe0816f950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbe0816f950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.2087\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6213\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 950us/step - loss: 0.4446\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3545\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3019\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2704\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 957us/step - loss: 0.2478\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2291\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2163\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbe08241890>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefered = Sequential(\n",
    "    [\n",
    "    Dense(25, activation = 'relu'),\n",
    "    Dense(15, activation = 'relu'),\n",
    "    Dense(4, activation = 'linear')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prefered.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "prefered.fit(\n",
    "    x_train, y_train,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Handling\n",
    "Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. Let's look at the preferred model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbde8170f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbde8170f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Two examples output vector : [[-4.143013    0.19964506  4.6349564   0.99905497]\n",
      " [ 3.7505262   0.38420558 -5.980793   -5.580508  ]]\n",
      "largest value:  12.087269\n",
      "smallest value:  -10.990598\n"
     ]
    }
   ],
   "source": [
    "pre_prefered = prefered.predict(x_train)\n",
    "print(f'Two examples output vector : {pre_prefered[:2]}')\n",
    "print('largest value: ', np.max(pre_prefered))\n",
    "print('smallest value: ',np.min(pre_prefered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[1.4839732e-04 1.1413490e-02 9.6305192e-01 2.5386205e-02]\n",
      " [9.6649694e-01 3.3360001e-02 5.7404002e-05 8.5661100e-05]]\n",
      "largest value 0.99999917 smallest value 7.245911e-10\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(pre_prefered).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4839732e-04 1.1413490e-02 9.6305192e-01 2.5386205e-02], category: 2\n",
      "[9.6649694e-01 3.3360001e-02 5.7404002e-05 8.5661100e-05], category: 0\n",
      "[0.86219364 0.13607714 0.00086485 0.00086435], category: 0\n",
      "[0.01046968 0.9214138  0.06356313 0.00455332], category: 1\n",
      "[0.00123136 0.59213495 0.40055788 0.00607582], category: 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{sm_prefered[i]}, category: {np.argmax(sm_prefered[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
